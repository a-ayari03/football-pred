{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 594,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "from mplsoccer import Pitch, Sbopen, VerticalPitch\n",
    "import os\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from tqdm import tqdm\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_https_data(season : str, league_id : str, league_name : str) : \n",
    "    \"\"\"\n",
    "    This Python function takes in three parameters: season, league_id, and league_name, which are all of type str. \n",
    "    The purpose of this function is to retrieve data from a specified URL on the fbref website using HTTPS protocol. \n",
    "    The URL is constructed using the league_id, season, and league_name parameters provided. \n",
    "    Once the URL is created, the function uses the requests library to send an HTTP GET request to the URL and retrieve the response data.\n",
    "    \"\"\"\n",
    "    url = f\"https://fbref.com/en/comps/{league_id}/{season}/schedule/{season}-{league_name}-Scores-and-Fixtures#sched_{season}_{league_id}_1\"\n",
    "    #url = \"https://fbref.com/en/comps/13/2021-2022/schedule/2021-2022-Ligue-1-Scores-and-Fixtures#sched_2021-2022_13_1\"\n",
    "    print(url)\n",
    "    with requests.Session() as s :\n",
    "        response = s.get(url, headers= headers , cookies=s.cookies, timeout = 10)\n",
    "        print(\"Request page Response:\", response)\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 581,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_game_detail_data(url, debug = False) :\n",
    "    \"\"\"\n",
    "    Fetches detailed game data from a given URL using the requests library and returns the response.\n",
    "\n",
    "    Args:\n",
    "    - url (str): The URL of the page to fetch game data from.\n",
    "    - debug (bool, optional): Whether to print debug information. Defaults to False.\n",
    "\n",
    "    Returns:\n",
    "    - response (Response): The response object obtained by making a GET request to the URL.\n",
    "\n",
    "    Raises:\n",
    "    - Exception: If the response status code is 429 (Too Many Requests).\n",
    "\n",
    "    Example usage:\n",
    "    response = fetch_game_detail_data(\"https://example.com/game_details\", debug=True)\n",
    "    \"\"\"\n",
    "    \n",
    "    with requests.Session() as s :\n",
    "        response = s.get(url, headers= headers , cookies=s.cookies, timeout = 10)\n",
    "        if debug :\n",
    "            print(url)\n",
    "            print(\"Request page Response:\", response)\n",
    "        \n",
    "        if response.status_code == 429 :\n",
    "            raise Exception(f\"Too many requests, you got timeout. Please wait 1 hour\")\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 702,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_table_schedule(table) :\n",
    "    \"\"\"\n",
    "    Parses a given HTML table containing soccer match schedules and returns a dictionary of match data.\n",
    "\n",
    "    Args:\n",
    "    - table (Tag): The HTML table object containing the match data.\n",
    "\n",
    "    Returns:\n",
    "    - data_table (dict): A dictionary containing the parsed match data, with each key being a row index and the corresponding value being a dictionary of match data.\n",
    "\n",
    "    Example usage:\n",
    "    soup = BeautifulSoup(html_doc, 'html.parser')\n",
    "    table = soup.find('table', {'id': 'match-schedule'})\n",
    "    data_table = parse_table_schedule(table)\n",
    "    \"\"\"\n",
    "    \n",
    "    data_table = {}\n",
    "    rows = table.findAll('tr')\n",
    "    for i,row in tqdm(enumerate(rows), total = len(rows)) :\n",
    "        try : # Handle empty row \n",
    "            data = {}\n",
    "            data['gameweek'] = row.find(['td','th'], {'data-stat': 'gameweek'}).text.strip()\n",
    "            data['dayofweek'] = row.find('td', {'data-stat': 'dayofweek'}).text.strip()\n",
    "            data['date'] = row.find('td', {'data-stat': 'date'}).text.strip()\n",
    "            data['start_time'] = row.find('td', {'data-stat': 'start_time'}).text.strip()\n",
    "            data['home_team'] = row.find('td', {'data-stat': 'home_team'}).text.strip()\n",
    "            data['home_xg'] = row.find('td', {'data-stat': 'home_xg'}).text.strip() if not TypeError else None\n",
    "            data['score'] = row.find('td', {'data-stat': 'score'}).text.strip()\n",
    "            data['away_xg'] = row.find('td', {'data-stat': 'away_xg'}).text.strip() if not TypeError else None\n",
    "            data['away_team'] = row.find('td', {'data-stat': 'away_team'}).text.strip()\n",
    "            data['attendance'] = row.find('td', {'data-stat': 'attendance'}).text.strip() if not TypeError else None\n",
    "            data['venue'] = row.find('td', {'data-stat': 'venue'}).text.strip()\n",
    "            data['game_detail'] = f\"https://fbref.com{row.find('td', {'data-stat': 'score'}).a['href']}\"\n",
    "            data_table[i] = data\n",
    "        except TypeError :\n",
    "            continue\n",
    "    return data_table "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 703,
   "metadata": {},
   "outputs": [],
   "source": [
    "season_list = ['2015-2016','2016-2017','2017-2018','2018-2019','2019-2020','2020-2021','2021-2022']\n",
    "league_table = {\n",
    "    'Ligue-1' : 13,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 704,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_agent = \"Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/535.1 (KHTML, like Gecko) Chrome/13.0.782.112 Safari/535.0\"\n",
    "headers = {\n",
    "    'User-Agent': user_agent,\n",
    "    'Accept-Encoding': 'gzip, deflate, br',\n",
    "    'Accept': '*/*',\n",
    "    'Connection': 'keep-alive'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## All season, single league "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 705,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2015-2016\n",
      "https://fbref.com/en/comps/13/2015-2016/schedule/2015-2016-Ligue-1-Scores-and-Fixtures#sched_2015-2016_13_1\n",
      "Request page Response: <Response [200]>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 422/422 [00:00<00:00, 2098.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2016-2017\n",
      "https://fbref.com/en/comps/13/2016-2017/schedule/2016-2017-Ligue-1-Scores-and-Fixtures#sched_2016-2017_13_1\n",
      "Request page Response: <Response [200]>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 426/426 [00:00<00:00, 1125.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-2018\n",
      "https://fbref.com/en/comps/13/2017-2018/schedule/2017-2018-Ligue-1-Scores-and-Fixtures#sched_2017-2018_13_1\n",
      "Request page Response: <Response [200]>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 424/424 [00:00<00:00, 1661.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-2019\n",
      "https://fbref.com/en/comps/13/2018-2019/schedule/2018-2019-Ligue-1-Scores-and-Fixtures#sched_2018-2019_13_1\n",
      "Request page Response: <Response [200]>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 432/432 [00:00<00:00, 1628.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-2020\n",
      "https://fbref.com/en/comps/13/2019-2020/schedule/2019-2020-Ligue-1-Scores-and-Fixtures#sched_2019-2020_13_1\n",
      "Request page Response: <Response [200]>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 421/421 [00:00<00:00, 1628.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-2021\n",
      "https://fbref.com/en/comps/13/2020-2021/schedule/2020-2021-Ligue-1-Scores-and-Fixtures#sched_2020-2021_13_1\n",
      "Request page Response: <Response [200]>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 427/427 [00:00<00:00, 1881.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-2022\n",
      "https://fbref.com/en/comps/13/2021-2022/schedule/2021-2022-Ligue-1-Scores-and-Fixtures#sched_2021-2022_13_1\n",
      "Request page Response: <Response [200]>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 425/425 [00:00<00:00, 1499.80it/s]\n"
     ]
    }
   ],
   "source": [
    "league_name = 'Ligue-1'\n",
    "league_id = league_table[league_name]\n",
    "df_list = []\n",
    "for season in season_list :\n",
    "    print(season)\n",
    "    r = fetch_https_data(season, league_id, league_name) \n",
    "    soup = BeautifulSoup(r.text, 'lxml') # parse html\n",
    "    table_row = soup.find('table').find('tbody') # focus on the table\n",
    "    data_parsed = parse_table_schedule(table_row)\n",
    "    df = pd.DataFrame.from_dict(data_parsed, orient = 'index')\n",
    "    df['season'] = season\n",
    "    df['league'] = league_name\n",
    "    df_list.append(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merge = pd.concat(df_list) # concatenate all season into a unique dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gather Game detail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 592,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_team_stats(teams_stats_table, debug = False) :\n",
    "    \"\"\"\n",
    "    Parses a given HTML table containing soccer team stats and returns a dictionary of selected stats.\n",
    "\n",
    "    Args:\n",
    "    - teams_stats_table (Tag): The HTML table object containing the team stats data.\n",
    "    - debug (bool): A flag indicating whether or not to print debug information during execution.\n",
    "\n",
    "    Returns:\n",
    "    - stats (dict): A dictionary containing the parsed team stats data, with each key being a selected statistic and the corresponding value being a tuple containing the home and away team values.\n",
    "\n",
    "    Example usage:\n",
    "    soup = BeautifulSoup(html_doc, 'html.parser')\n",
    "    table = soup.find('table', {'id': 'team-stats'})\n",
    "    stats = parse_team_stats(table)\n",
    "    \"\"\"\n",
    "    stats_header = [\"Possession\", \"Shots on Target\", \"Saves\"]\n",
    "    stats = {}\n",
    "    teams_stats_table\n",
    "    tr = teams_stats_table.findAll('tr')\n",
    "    tr = tr[1::] # first value is irrelevant\n",
    "    # header and value got distinct tr. So we link them in a tuple \n",
    "    tr_header = tr[::2] # every 2 tr we got the name of the value\n",
    "    tr_values = tr[1::2] # same \n",
    "    \n",
    "    tr = [(h,v) for h,v in zip(tr_header,tr_values)]\n",
    "    i = 0\n",
    "    for _tr in tr : \n",
    "        try :\n",
    "            #print(_tr)\n",
    "            _tr_header, _tr_value  = _tr \n",
    "            td = _tr_value.find_all('td') # contain one stat type for 2 teams\n",
    "            \n",
    "            value = []\n",
    "            for j,_td in enumerate(td) : # loop throw team value\n",
    "                val = _td.find('div').find('div')\n",
    "                value.append(val)\n",
    "            if debug :\n",
    "                print(value)\n",
    "                print('-'*15)\n",
    "            header = _tr_header.text.strip()\n",
    "            if header in stats_header : # we only want specific stats\n",
    "                stats[f'{header} home'] = value[0]\n",
    "                stats[f'{header} away'] = value[1]\n",
    "                i += 1\n",
    "        except IndexError: # handle error for td with only the stats name\n",
    "            continue\n",
    "    return stats\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 571,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_team_detail(url) :\n",
    "    \"\"\"\n",
    "    Fetches detailed game statistics data from a given URL and returns them as a dictionary.\n",
    "\n",
    "    Args:\n",
    "    - url (str): The URL from which to fetch the data.\n",
    "\n",
    "    Returns:\n",
    "    - stats (dict): A dictionary containing the parsed game statistics data for the specified URL.\n",
    "\n",
    "    Example usage:\n",
    "    url = \"https://fbref.com/en/matches/5e7d1b5c/Sheffield-United-Manchester-City-March-17-2020-Premier-League\"\n",
    "    stats = get_team_detail(url)\n",
    "    \"\"\"\n",
    "    r_detail = fetch_game_detail_data(url)\n",
    "    soup_detail = BeautifulSoup(r_detail.text, 'lxml') # parse html\n",
    "    div = soup_detail.find(\"div\", {\"id\" :\"team_stats\"})\n",
    "    stats = parse_team_stats(div)\n",
    "    \n",
    "    return stats "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "r = fetch_game_detail_data(_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "soup_detail = BeautifulSoup(r.text, 'lxml') # parse html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "div = soup_detail.find(\"div\", {\"id\" :\"team_stats\"})\n",
    "stats = parse_team_stats(div)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 672,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{}"
      ]
     },
     "execution_count": 672,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 673,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                         | 0/2670 [00:04<?, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "url_list = df_merge[df_merge[\"possession_home\"].isnull()][\"game_detail\"].values.tolist() # only games without values\n",
    "url_list = df_merge[\"game_detail\"].values.tolist() # only games without values\n",
    "game_detail = {}\n",
    "for _url in tqdm(url_list) :\n",
    "    stats = get_team_detail(_url)\n",
    "    #game_detail[_url] = stats\n",
    "    if stats : # if dictionnary is null, no values to be added\n",
    "        df_merge = new_features_game_detail(df_merge, _url, stats)\n",
    "    time.sleep(2.5) #timeout\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 640,
   "metadata": {},
   "outputs": [],
   "source": [
    "def new_features_game_detail(df, url, stats_dict) :\n",
    "    \n",
    "    df.loc[df.game_detail == url,\"possession_home\"] = str(stats_dict[\"Possession home\"])\n",
    "    df.loc[df.game_detail == url,\"possession_away\"] = str(stats_dict[\"Possession away\"])\n",
    "    df.loc[df.game_detail == url,\"shot_on_target_home\"] = str(stats_dict[\"Shots on Target home\"])\n",
    "    df.loc[df.game_detail == url,\"shot_on_target_away\"] = str(stats_dict[\"Shots on Target away\"])\n",
    "    df.loc[df.game_detail == url,\"saves_home\"] = str(stats_dict[\"Saves home\"])\n",
    "    df.loc[df.game_detail == url,\"saves_away\"] = str(stats_dict[\"Saves away\"])\n",
    "    \n",
    "    return df "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save to local file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 711,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merge.to_csv(f\"{league_name}-2015-2022.csv\", index = False, encoding = 'utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 643,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_merge[['possession_home', 'possession_away', 'shot_on_target_home', 'shot_on_target_away', 'saves_home', 'saves_away']] = None"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
